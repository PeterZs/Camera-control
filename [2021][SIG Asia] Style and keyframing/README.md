# Camera Keyframing with Style and Control

This repo provides PyTorch implementation of our paper :

*Camera Keyframing with Style and Control*

[Hongda Jiang](https://jianghd1996.github.io/), [Marc Christie](http://people.irisa.fr/Marc.Christie/), [Xi Wang](https://triocrossing.github.io/), [Libin Liu](http://libliu.info/), [Bin Wang](https://sites.google.com/view/wangbin/), [Baoquan Chen](https://cfcs.pku.edu.cn/baoquan/)

ACM SIGGRAPH Asia 2021



The homepage of the project can be found [here](https://jianghd1996.github.io/publication/siga_2021/). The paper can be found [here](https://jianghd1996.github.io/publication/siga_2021/SIGA_2021.pdf).



**To do list**

- [ ] test scene



## Prerequisites

- Linux
- NVIDIA GPU + CUDA CuDNN
- Python 3.6.9



## Dependencies

```bash
pip install -r requirements.txt
```



 ## Dataset

The data we use is consists of two parts:

- The first is part is generated by Unity, where we run the camera control scripts in multiple scenes with variant trajectories. And for each trajectory, render results of each frame will be in the data file which consists three parts.

1. Camera toric space (pA, pB, pY, theta, phi)
2. Character A info
3. Character B info

For character information, the 2D screen coordinate and 3D coordinate of each joints will be listed.

- The second part is extracted from [MovieNet](http://movienet.site/) with the [cinematic feature estimator tool](https://github.com/jianghd1996/Camera-control/tree/master/SIGGRAPH_2020). We would release the data after accessing the permission.





## Pretrained Model

Pretrained camera keyframing model  

[[google drive](https://drive.google.com/file/d/1-ulS9hXV1T0FjlWZAo2uAbYe8V__Lntq/view?usp=sharing)]     [[baiduyun](https://pan.baidu.com/s/1bgyuupD0-CaeEH5AE_I6aQ)] (password: 2vp3)



## Dataset Download

We prepare pre-processed  data for training :

Estimation dataset [[training](https://drive.google.com/drive/folders/1WSKigQQkuyav_iZdl6ujenNQDqPZiNq7?usp=sharing)] [[testing](https://drive.google.com/drive/folders/1KyxQniLAyhBO1lYdv3dr8ok6OR0BdnU9?usp=sharing)]

Prediction dataset [[training](https://drive.google.com/drive/folders/1J_mHUEZxou8vymyeEIqbFQL4DWWgEq5Q?usp=sharing)] [[testing](https://drive.google.com/drive/folders/1q2A92U0OzMQkJOEZiAWMZvz7yjfWsmy4?usp=sharing)]

**You can also generate your own dataset with the project in folder data_generation**



## Testing

In test time, we extract camera behaviors in a reference real film clips and retarget it to a new 3D animation. The process has 3 steps. 



**Step 1** is the cinematic feature estimation. 

According to our paper, this part has three steps :

1. Use [LCR-Net](http://lear.inrialpes.fr/src/LCR-Net/) to estimate 2D skeletons from videos
2. Pose association, smoothing and filling missing joints
3. Estimate cinematic features with a neural network



**Step 2** is mapping the film clips to a latent camera behaviors space.

Use the cinematic feature from step 1, input them to the pretrained Gating network to get a sequence of camera behaviors vector.



**Step 3** is apply the extracted behaviors to a new 3D animation.

Use the vector in step 2 to control the weights of Prediction network in every step, input the scene content to the prediction network to get the camera pose of next frame.



**Please refer to folder Movie_Analysis for detail usage.**



## Acknowledgments


This work was supported in part by the National Key R&D Program of China (2018YFB1403900, 2019YFF0302902).



## Citation


Please cite our work if you find it useful:

```
@article{jiang2020example,
  title={Example-driven Virtual Cinematography by Learning Camera Behaviors},
  author={Hongda, Jiang and Bin, Wang and Xi, Wang and Marc, Christie and Baoquan Chen},
  journal={ACM Transactions on Graphics (TOG)},
  year={2020},
  publisher={ACM New York, NY, USA}
}
```

