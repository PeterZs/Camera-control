# Example-driven Virtual Cinematography by Learning Camera Behaviors 

This repo provides PyTorch implementation of our paper :

*Example-driven Virtual Cinematography by Learning Camera Behaviors*

[Hongda Jiang](https://jianghd1996.github.io/)\*, [Bin Wang](https://sites.google.com/view/wangbin/)\*, [Xi Wang](https://triocrossing.github.io/), [Marc Christie](http://people.irisa.fr/Marc.Christie/), [Baoquan Chen](https://cfcs.pku.edu.cn/baoquan/)

ACM SIGGRAPH 2020

![Overview](https://github.com/jianghd1996/Camera-control/blob/master/SIGGRAPH_2020/Figure/teaser.png)

The homepage of the project can be found [here](https://jianghd1996.github.io/publication/sig_2020/). The paper can be found [here](https://jianghd1996.github.io/publication/sig_2020/SIG_2020.pdf).



**To do list**

- [ ] Usage

- [ ] test scene



## Prerequisites

- Linux
- NVIDIA GPU + CUDA CuDNN
- Python 3.6



## Dependencies

```bash
pip install -r requirements.txt
```



 ## Dataset

The data we use is generated by Unity, where we run the camera control scripts in multiple scenes with variant trajectories. And for each trajectory, render results of each frame will be in the data file which consists three parts.

1. Camera toric space (pA, pB, pY, theta, phi)
2. Character A info
3. Character B info

For character information, the 2D screen coordinate and 3D coordinate of each joints will be listed.



## Cinematic Feature Space

We define the cinematic feature space as correlation between characters and cameras.

For character features, we use character distance, relative head orientations, relative shoulder orientations and main character information. For camera features, we use the toric camera space.

![Overview](https://github.com/jianghd1996/Camera-control/blob/master/SIGGRAPH_2020/Figure/feature.png)

Use data_processing file to extract the character 2D screen joint positions and cinematic features.



## Pipeline

Our framework have two stages, cinematic feature estimator and camera motion controller.

**Stage 1** cinematic feature estimator input character 2D screen positions to estimate cinematic features. The model includes a CNN feature extractor to compress the information in continuous 8 frames, and four different fully connected blocks to regress cinematic features.

**Please refer to Cinematic_Estimation folder for detail usage**



**Stage 2** camera motion controller use a Mixture of Experts method which is a Gating+Prediction mechanism. 

The input to Gating is cinematic features of the reference clips, the Gating is LSTM model, and for each frame input that cinematic features. The output of Gating network will control the weight of Prediction network, which is achieved by a combination of multiple Linear layers in PyTorch.

The input to Prediction is character features in history and future, as well as history camera features, and the output is the predicted current camera features.

**Please refer to Gating_Prediction folder for detail usage**



## Pretrained Model

Pretrained cinematic estimator model

[[google drive](https://drive.google.com/file/d/1PpAKJk8OYqP1m_oMr4DhfHDZiGrN7MQV/view?usp=sharing)]  

Pretrained MoE model (number of expert = 9) 

[[google drive](https://drive.google.com/file/d/1-ulS9hXV1T0FjlWZAo2uAbYe8V__Lntq/view?usp=sharing)]     [[baiduyun](https://pan.baidu.com/s/1bgyuupD0-CaeEH5AE_I6aQ)] (password: 2vp3)



## Dataset Download

We prepare pre-processed  data for training :

Estimation dataset [[training](https://drive.google.com/drive/folders/1WSKigQQkuyav_iZdl6ujenNQDqPZiNq7?usp=sharing)] [[testing](https://drive.google.com/drive/folders/1KyxQniLAyhBO1lYdv3dr8ok6OR0BdnU9?usp=sharing)]

Prediction dataset [[training](https://drive.google.com/drive/folders/1J_mHUEZxou8vymyeEIqbFQL4DWWgEq5Q?usp=sharing)] [[testing](https://drive.google.com/drive/folders/1q2A92U0OzMQkJOEZiAWMZvz7yjfWsmy4?usp=sharing)]

**You can also generate your own dataset with the project in folder data_generation**



## Testing

In test time, we extract camera behaviors in a reference real film clips and retarget it to a new 3D animation. The process has 3 steps. 



**Step 1** is the cinematic feature estimation. 

According to our paper, this part has three steps :

1. Use [LCR-Net](http://lear.inrialpes.fr/src/LCR-Net/) to estimate 2D skeletons from videos
2. Pose association, smoothing and filling missing joints
3. Estimate cinematic features with a neural network



**Step 2** is mapping the film clips to a latent camera behaviors space.

Use the cinematic feature from step 1, input them to the pretrained Gating network to get a sequence of camera behaviors vector.



**Step 3** is apply the extracted behaviors to a new 3D animation.

Use the vector in step 2 to control the weights of Prediction network in every step, input the scene content to the prediction network to get the camera pose of next frame.



**Please refer to folder Movie_Analysis for detail usage.**



## Acknowledgments


This work was supported in part by the National Key R&D Program of China (2018YFB1403900, 2019YFF0302902).



## Citation


Please cite our work if you find it useful:

```
@article{jiang2020example,
  title={Example-driven Virtual Cinematography by Learning Camera Behaviors},
  author={Hongda, Jiang and Bin, Wang and Xi, Wang and Marc, Christie and Baoquan Chen},
  journal={ACM Transactions on Graphics (TOG)},
  year={2020},
  publisher={ACM New York, NY, USA}
}
```

